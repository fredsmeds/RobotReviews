{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-17 13:53:11.959757: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-17 13:53:11.959818: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-17 13:53:11.961141: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-17 13:53:11.969305: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-17 13:53:12.934640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfbf011cd9f42a98b01aecb1480e289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c36d2008ddc48209d84b3369c364ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd62c11833c941fda29a196fb5170c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7ad08e6edf484e8a12d1706647380e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6344ae6ee9a4e3fa68d55edbb28bab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12775' max='12775' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12775/12775 37:38, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.694300</td>\n",
       "      <td>0.553215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.512500</td>\n",
       "      <td>0.465080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.485600</td>\n",
       "      <td>0.411837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.343100</td>\n",
       "      <td>0.382135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.405200</td>\n",
       "      <td>0.370627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12775' max='12775' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12775/12775 37:24, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.433300</td>\n",
       "      <td>0.289155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>0.243419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.282300</td>\n",
       "      <td>0.210019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.345800</td>\n",
       "      <td>0.198603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.247400</td>\n",
       "      <td>0.194467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12775' max='12775' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12775/12775 37:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.258600</td>\n",
       "      <td>0.146135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.200600</td>\n",
       "      <td>0.127005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.167800</td>\n",
       "      <td>0.119983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.168500</td>\n",
       "      <td>0.115293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.198500</td>\n",
       "      <td>0.111018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Accuracy: 0.9198147060742481\n",
      "Avg Precision: 0.920401176453891\n",
      "Avg Recall: 0.9198147060742481\n",
      "Avg F1 score: 0.9197869333589356\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable WandB\n",
    "\n",
    "# Load the dataset\n",
    "file_path = '/notebooks/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "columns_to_keep = ['name', 'brand', 'primaryCategories', 'reviews.text', 'reviews.rating']\n",
    "df_selected = df[columns_to_keep]\n",
    "\n",
    "# Balance the dataset\n",
    "positive_reviews = df_selected[df_selected['reviews.rating'] >= 4]\n",
    "neutral_reviews = df_selected[df_selected['reviews.rating'] == 3]\n",
    "negative_reviews = df_selected[df_selected['reviews.rating'] <= 2]\n",
    "\n",
    "max_class_size = max(len(positive_reviews), len(neutral_reviews), len(negative_reviews))\n",
    "positive_upsampled = resample(positive_reviews, replace=True, n_samples=max_class_size, random_state=42)\n",
    "neutral_upsampled = resample(neutral_reviews, replace=True, n_samples=max_class_size, random_state=42)\n",
    "negative_upsampled = resample(negative_reviews, replace=True, n_samples=max_class_size, random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([positive_upsampled, neutral_upsampled, negative_upsampled]).sample(frac=1, random_state=42)\n",
    "\n",
    "df_balanced['labels'] = df_balanced['reviews.rating'].map(lambda rating: 0 if rating <= 2 else (1 if rating == 3 else 2))\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_balanced['reviews.text'], df_balanced['labels'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "\n",
    "# Create HuggingFace Datasets for the train and test sets\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_encodings['input_ids'].tolist(),\n",
    "    'attention_mask': train_encodings['attention_mask'].tolist(),\n",
    "    'labels': y_train.tolist()\n",
    "})\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"]\n",
    ")\n",
    "\n",
    "# Load DistilBERT model with LoRA\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,  # You can adjust the number of epochs\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1,\n",
    "    no_cuda=False  # Use GPU if available\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "# Cross-Validation using 5 folds for more accurate evaluation\n",
    "kf = KFold(n_splits=3)  # Changed from 2 to 5 folds\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "    print(f\"Running Fold {fold + 1}\")\n",
    "\n",
    "    # Create fold-specific datasets\n",
    "    X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    # Tokenize the fold training and validation data\n",
    "    fold_train_encodings = tokenizer(X_fold_train.tolist(), truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "    fold_val_encodings = tokenizer(X_fold_val.tolist(), truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "\n",
    "    # Create fold-specific datasets\n",
    "    fold_train_dataset = Dataset.from_dict({\n",
    "        'input_ids': fold_train_encodings['input_ids'].tolist(),\n",
    "        'attention_mask': fold_train_encodings['attention_mask'].tolist(),\n",
    "        'labels': y_fold_train.tolist()\n",
    "    })\n",
    "\n",
    "    fold_val_dataset = Dataset.from_dict({\n",
    "        'input_ids': fold_val_encodings['input_ids'].tolist(),\n",
    "        'attention_mask': fold_val_encodings['attention_mask'].tolist(),\n",
    "        'labels': y_fold_val.tolist()\n",
    "    })\n",
    "\n",
    "    # Update trainer datasets for this fold\n",
    "    trainer.train_dataset = fold_train_dataset\n",
    "    trainer.eval_dataset = fold_val_dataset\n",
    "\n",
    "    # Fine-tune the model on the fold training data\n",
    "    trainer.train()\n",
    "\n",
    "    # Predict for the validation set\n",
    "    predictions = trainer.predict(fold_val_dataset)\n",
    "\n",
    "    # Convert logits to predictions\n",
    "    preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_fold_val, preds)\n",
    "    precision = precision_score(y_fold_val, preds, average='weighted')\n",
    "    recall = recall_score(y_fold_val, preds, average='weighted')\n",
    "    f1 = f1_score(y_fold_val, preds, average='weighted')\n",
    "\n",
    "    fold_metrics.append({\n",
    "        'fold': fold + 1,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    })\n",
    "\n",
    "# Calculate and print average metrics\n",
    "avg_accuracy = np.mean([f['accuracy'] for f in fold_metrics])\n",
    "avg_precision = np.mean([f['precision'] for f in fold_metrics])\n",
    "avg_recall = np.mean([f['recall'] for f in fold_metrics])\n",
    "avg_f1 = np.mean([f['f1'] for f in fold_metrics])\n",
    "\n",
    "print(f\"Avg Accuracy: {avg_accuracy}\")\n",
    "print(f\"Avg Precision: {avg_precision}\")\n",
    "print(f\"Avg Recall: {avg_recall}\")\n",
    "print(f\"Avg F1 score: {avg_f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model and tokenizer saved to C:\\Users\\fmrol\\Documents\\GitHub\\RobotReviews\\fredsmeds_classifier\n"
     ]
    }
   ],
   "source": [
    "# Save the final model and tokenizer after all cross-validation and training is done\n",
    "output_dir = r\"C:\\Users\\fmrol\\Documents\\GitHub\\RobotReviews\\fredsmeds_classifier\"\n",
    "trainer.save_model(output_dir)  # Save the fine-tuned model\n",
    "tokenizer.save_pretrained(output_dir)  # Save the tokenizer\n",
    "\n",
    "print(f\"Final model and tokenizer saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./saved_model/fredsmeds_classifier')\n",
    "\n",
    "# Saving the tokenizer with the same custom name\n",
    "tokenizer.save_pretrained('./saved_model/fredsmeds_classifier')\n",
    "\n",
    "# Optionally, save the training arguments if needed\n",
    "training_args.save('./saved_model/fredsmeds_classifier/training_args.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
